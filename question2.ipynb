{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Q2 BOOLEAN QUERIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vBs3OxqJXu77"
      },
      "outputs": [],
      "source": [
        "set_=set()\n",
        "set1=set()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) INVERTED INDEX OF THE DATA OBTAINED FROM Q(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_fmlEBo6KP2"
      },
      "outputs": [],
      "source": [
        "# First part of second question\n",
        "def unigram(x,l):\n",
        "  Str = x\n",
        "  N = 13\n",
        "  # create a new string of last N characters\n",
        "  Str2 = Str[-N:]\n",
        "  # print Last N characters\n",
        "  filename1=Str2\n",
        "  m=[]\n",
        "  # If the word does not exist, add it to the list\n",
        "  with open(x,'r') as file:\n",
        "    for word in file:\n",
        "      if word not in m:\n",
        "        m.append(word)\n",
        "  s=m[0] \n",
        "  s=s.split()\n",
        "  # print(s)\n",
        "  # Creating a dictionary using for loops\n",
        "  # The first loop adds the word and the first document it exists in to the unigram dictionary\n",
        "  for word in s:\n",
        "    if l.get(word) is None:\n",
        "      l[word]=[filename1]\n",
        "    \n",
        "    else:\n",
        "      # This for loop adds the subsequent documents that the word exists in after checking if the word already exists in the dictionary with its first document\n",
        "      for w in l.get(word):\n",
        "        if filename1 not in l.get(word):\n",
        "          set_.add(filename1)\n",
        "          set1.add(filename)\n",
        "          l.get(word).append(filename1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) PYTHON'S PICKLE MODULE TO SAVE AND LOAD THE UNIGRAM INVERTED INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KmWnkJ1ZNVLg"
      },
      "outputs": [],
      "source": [
        "# second part of second question\n",
        "import pickle\n",
        "def pickle_module(dir):\n",
        "  #  The pickle module is used to dump the files\n",
        "  with open('dir.pkl', 'wb') as f:\n",
        "    pickle.dump(dir, f)\n",
        "    print(dir)\n",
        "  return dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# assign directory\n",
        "directory = 'C://Users//ECOO33AU//OneDrive//Desktop//CSE508_Winter2023_Dataset//CSE508_Winter2023_Dataset//'\n",
        "l={}\n",
        "# Main that calls the unigram function for all the documents\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        unigram(f,l)\n",
        "x=pickle_module(l)\n",
        "# print(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) VARIOUS OPERATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# third part of second question\n",
        "def AND(t1,t2,dir):\n",
        "  z=set()\n",
        "  if t1 is not None and t2 is not None:\n",
        "    x=set((t1))\n",
        "    y=set((t2))\n",
        "    z = x.intersection(y)\n",
        "  return list(z)\n",
        "\n",
        "def AND_NOT(t1,t2,dir):\n",
        "  z=set()\n",
        "\n",
        "  if t2 is not None and t1 is not None:\n",
        "    x=set((t1))\n",
        "    y=set((t2))\n",
        "    y=set_ - y\n",
        "    print(y)\n",
        "    z= x.intersection(y)    \n",
        "  elif t2 is None and t1 is not None:\n",
        "    x=set(t1)\n",
        "    z=x.intersection(set_)\n",
        "    print(z)\n",
        "  \n",
        "  return list(z)\n",
        "\n",
        "def OR(t1,t2,dir):\n",
        "  z=set()\n",
        "  if t1 is not None and t2 is not None:\n",
        "    x=set(t1)\n",
        "    y=set(t2)\n",
        "    x=x-y\n",
        "    y=y-x\n",
        "    z=x.union(y)\n",
        "  elif t1 is None and t2 is not None:\n",
        "\n",
        "    z= set(t2)\n",
        "  elif t2 is None and t1 is not None:\n",
        "    z= set(t1)\n",
        "  \n",
        "  return list(z)\n",
        "\n",
        "def OR_NOT(t1,t2,dir):\n",
        "  z=set()\n",
        "  if t2 is not None and t1 is not None:\n",
        "    x=set(t1)\n",
        "    y=set(t2)\n",
        "    y=set_-y\n",
        "    z=OR(list(x),list(y),dir)\n",
        "  if t2 is None and t1 is not None:\n",
        "    z=set_-set((t1))\n",
        "  if t1 is None and t2 is None:\n",
        "    return set_\n",
        "  return list(z) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) GENERALIZED FUNCTION FOR ALL QUERIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fourth part of second question\n",
        "def Generalized_Functions(n,input_seq,operations,dir):\n",
        "  count=0\n",
        "  operations=operations.upper()\n",
        "  operations = operations.split(',')\n",
        "  if len(operations)!=n:\n",
        "    print(len(operations))\n",
        "    print(n)\n",
        "    print(\"The number of queries do not match the operations\")\n",
        "    return \n",
        "  input_seq = input_seq.split()\n",
        "  length = len(input_seq)\n",
        "  if (length - len(operations))!=1:\n",
        "    print(\"Number of operations does not match the input sequence\")\n",
        "    return\n",
        "  extra = []\n",
        "  x=0\n",
        "  for i in range(length):\n",
        "    extra.append(dir.get(input_seq[i]))\n",
        "\n",
        "  for i in range(length-1):\n",
        "    if x==n:\n",
        "      break\n",
        "    if operations[x]==\"AND\":\n",
        "      l=AND(extra[i],extra[i+1],dir) \n",
        "      count=count+len(extra[i])+len(extra[i+1])\n",
        "      extra[i+1] = l\n",
        "      \n",
        "    if operations[x]==\"OR\":\n",
        "      \n",
        "      l=OR(extra[i],extra[i+1],dir) \n",
        "      if(extra[i]<extra[i+1]):\n",
        "        count=count+len(extra[i+1])\n",
        "      else:\n",
        "        count=count+len(extra[i])\n",
        "      extra[i+1] = l\n",
        "      \n",
        "    if operations[x]==\"AND NOT\":\n",
        "      l=AND_NOT(extra[i],extra[i+1],dir) \n",
        "      count=count+len(extra[i])+len(extra[i+1])\n",
        "      extra[i+1] = l\n",
        "    \n",
        "    if operations[x]==\"OR NOT\":\n",
        "      \n",
        "      l=OR_NOT(extra[i],extra[i+1],dir) \n",
        "      if(extra[i]<extra[i+1]):\n",
        "        count=count+len(extra[i+1])\n",
        "      else:\n",
        "        count=count+len(extra[i])\n",
        "      extra[i+1] = l\n",
        "    x=x+1\n",
        "\n",
        "  return [extra[len(extra)-1],len(extra[len(extra)-1]),count]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. computation of minimum numbers of  operation for performing query is done in question 4 only"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6) TAKING INPUT FROM USER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# taking number of testcases as user input\n",
        "x = int(input())\n",
        "data= []\n",
        "count=0\n",
        "\n",
        "while(x>0):\n",
        "    y = input()\n",
        "    if(len(y.split())<=5):\n",
        "        x=x-1\n",
        "        # storing the user input in list named data\n",
        "        data.append(y)\n",
        "    else:\n",
        "        # returning error if the statement length is >5\n",
        "        print(\"Enter input of length <=5 \")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-> PREPROCESSING THE INPUT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IMPORTING LIBRARIES REQUIRED FOR PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ECOO33AU\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ECOO33AU\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERTING TO LOWERCASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['heating rate may occur', 'similar solutions compressible']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [i.lower() for i in data]\n",
        "data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REMOVING PUNCTUATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['heating rate may occur', 'similar solutions compressible']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator = str.maketrans('', '', string.punctuation)\n",
        "data=[i.translate(translator) for i in data]\n",
        "data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REMOVING STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['heating', 'rate', 'may', 'occur'], ['similar', 'solutions', 'compressible']]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_stopwords(text):\n",
        "\tstop_words = set(stopwords.words(\"english\"))\n",
        "\tword_tokens = word_tokenize(text)\n",
        "\tfiltered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\treturn filtered_text\n",
        "\n",
        "data1=[]\n",
        "for i in data:\n",
        "    data1.append(remove_stopwords(i))\n",
        "    \n",
        "data1    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERTING LIST BACK TO STRING FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['heating rate may occur ', 'similar solutions compressible ']\n"
          ]
        }
      ],
      "source": [
        "data2=[]\n",
        "\n",
        "for i in data1:\n",
        "    s=\"\"\n",
        "    for j in i:\n",
        "        s+=(j + ' ')\n",
        "    data2.append(s)\n",
        "        \n",
        "print(data2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REMOVING BLANK SPACES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['heating rate may occur', 'similar solutions compressible']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = [\" \".join(i.split()) for i in data2]\n",
        "data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TAKING THE NUMBER OF OPERATIONS AS INPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "opn=[]\n",
        "for i in data:\n",
        "    temp=[]\n",
        "    for j in range(len(i.split())-1):\n",
        "        y = input(\"Enter operations:\")\n",
        "        temp.append(y)\n",
        "    opn.append(temp)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7(1) PRINTING THE WHOLE QUERY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "st=[]\n",
        "k1=0\n",
        "for i in data:\n",
        "    temp=[]\n",
        "    k=0 \n",
        "    for j in (i.split()[:-1]):\n",
        "        temp.append(j)\n",
        "        temp.append(opn[k1][k].upper())\n",
        "        k=k+1\n",
        "    temp.append(i.split()[-1])\n",
        "    st.append(temp)\n",
        "    k1=k1+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['heating', 'AND', 'rate', 'AND', 'may', 'AND', 'occur'], ['similar', 'AND', 'solutions', 'AND', 'compressible']]\n",
            "heating AND rate AND may AND occur \n",
            "similar AND solutions AND compressible \n"
          ]
        }
      ],
      "source": [
        "print(st)\n",
        "string_ans=[]\n",
        "for i in st:\n",
        "    s=\"\"\n",
        "    for j in i:\n",
        "        s+=(j + ' ')\n",
        "    print(s)\n",
        "    string_ans.append(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query 1: heating AND rate AND may AND occur \n",
            "Number of documents retrived for query 1: 1\n",
            "Name of documents retrived for query 1: ['cranfield0005']\n",
            "Number of comparisions required for query 1: 483\n",
            "                 \n",
            "Query 2: similar AND solutions AND compressible \n",
            "Number of documents retrived for query 2: 12\n",
            "Name of documents retrived for query 2: ['cranfield0629', 'cranfield0565', 'cranfield0562', 'cranfield0305', 'cranfield0011', 'cranfield0072', 'cranfield0309', 'cranfield0118', 'cranfield0941', 'cranfield0094', 'cranfield0073', 'cranfield0062']\n",
            "Number of comparisions required for query 2: 463\n",
            "                 \n"
          ]
        }
      ],
      "source": [
        "# print(string_ans)\n",
        "k=0\n",
        "answer=[]\n",
        "for i in data:\n",
        "    length=len(i.split())\n",
        "    s = \"\"\n",
        "    for j in opn[k][:-1]:\n",
        "        s+=j\n",
        "        s+=','\n",
        "    s+=opn[-1][k]\n",
        "    x=Generalized_Functions(length-1,i,s,l)\n",
        "    k=k+1\n",
        "    answer.append(x)\n",
        "    \n",
        "\n",
        "\n",
        "count=1\n",
        "for i in range(len(string_ans)):\n",
        "    print(\"Query {}:\".format(count),string_ans[i])\n",
        "    print(\"Number of documents retrived for query {}:\".format(count),answer[i][1])\n",
        "    print(\"Name of documents retrived for query {}:\".format(count),answer[i][0])\n",
        "    print(\"Number of comparisions required for query {}:\".format(count),answer[i][2])\n",
        "    print('                 ')\n",
        "    count+=1\n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b43dddb97dd0035d8d45d3db5e4e252852c7d746c9115980eacd8ad3cdd2123"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
